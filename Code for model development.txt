# -*- coding: utf-8 -*-
"""
Created on Fri May 30 00:49:18 2025

@author: Administrator
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    classification_report, roc_auc_score, roc_curve, precision_recall_curve, 
    auc, f1_score, accuracy_score, precision_score, recall_score, average_precision_score
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from xgboost import XGBClassifier
from sklearn.preprocessing import label_binarize
from sklearn.utils import resample
from collections import Counter, defaultdict
from imblearn.over_sampling import SMOTE 
from sklearn.feature_selection import RFECV
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
import os
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss
import shap
from sklearn.inspection import PartialDependenceDisplay
import warnings

# Ignore warnings
warnings.filterwarnings('ignore')

# Set plot configurations
plt.rcParams['figure.dpi'] = 300       
plt.rcParams['savefig.dpi'] = 600     
plt.rcParams['font.family'] = 'Simhei'  
plt.rcParams['axes.linewidth'] = 1.2   
plt.rcParams['font.size'] = 12  # Set SHAP parameters
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.linewidth'] = 1.2
plt.rcParams['font.size'] = 12

def compute_ece(y_true, y_prob, n_bins=10):
    """
    Calculate Expected Calibration Error (ECE)
    """
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(y_prob, bin_boundaries[:-1])
    
    ece = 0.0
    for bin_idx in range(n_bins):
        mask = bin_indices == bin_idx
        if np.sum(mask) == 0:
            continue
        bin_probs = y_prob[mask]
        bin_acc = np.mean(y_true[mask])
        bin_conf = np.mean(bin_probs)
        ece += np.abs(bin_conf - bin_acc) * len(bin_probs)
    
    return ece / len(y_prob)

def multiclass_brier(y_true, y_prob):
    """
    Compute Multiclass Brier Score
    """
    y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
    return np.mean(np.sum((y_prob - y_true_bin) ** 2, axis=1))

'''
Data Loading and Type Conversion
'''
data = pd.read_csv('D://Anaconda//39_features_3class_data.csv')
# Column name mapping
column_mapping = {
    "X9": "BMI",
    "X8": "Age",
    "X5": "Reduced Energy Intake",
    "X15": "Neutrophil Count",
    "X28": "Magnesium Ions",
    "X34": "Total Bilirubin",
    "X30": "Platelet Count",
    "X24": "PaO2",
    "X32": "Serum Creatinine",
    "X19": "IL-6",
    "X22": "hs-CRP",
    "X10": "Total Protein",
    "X18": "Fasting Blood Glucose Value",
    "X12": "Hemoglobin",
    "X11": "Albumin",
    "X23": "pH Value",
    "X26": "Sodium Ions"
}

data.rename(columns=column_mapping, inplace=True)
print(data.head())
print(data.info())

# Convert object types to numeric
object_columns = data.select_dtypes(include=['object']).columns
for col in object_columns:
    data[col] = pd.to_numeric(data[col], errors='coerce')

print(data)
data.info()
Variable_Name = data.columns 

'''
Missing Value Imputation using Random Forest
'''
print(pd.isna(data).sum())

# Missing value visualization
import missingno as msno
msno.matrix(data, figsize=(16, 10), width_ratios=(13, 2), color=(0.25, 0.25, 0.5))
plt.show()

# Drop columns with <30 non-null values
data = data.dropna(axis='columns', thresh=30)
print(data.dtypes)

# List of variables without missing values
estimate = ['X1', 'X2', 'X3', 'X4',
            'Reduced Energy Intake',  'X6', 'X7', 'Age',  'Total Protein', 'Albumin',
            'Hemoglobin', 'X13', 'X14', 'Neutrophil Count',
            'X17', 'Fasting Blood Glucose Value',
            'X25', 'Sodium Ions', 'X27', 'Platelet Count', 'X31', 'Serum Creatinine', 'X33', 'Total Bilirubin',
            'X35', 'X36', 'X37', 'X38', 'X39', 'Y']


# Random forest filling missing values
def set_missing(df, estimate_list, miss_col):
    """Perform missing value imputation using Random Forest
  Args:
      df: Input DataFrame
      feature_list: List of predictive features
      target_col: Target column with missing values
    """
    col_list = estimate_list
    col_list.append(miss_col)
    process_df = df.loc[:, col_list]
    class_le = LabelEncoder()
    for i in col_list[:-1]:
        process_df.loc[:, i] = class_le.fit_transform(process_df.loc[:, i].values)
    # Split known/unknown samples
    known = process_df[process_df[miss_col].notnull()].values
    known[:, -1] = class_le.fit_transform(known[:, -1])
    unknown = process_df[process_df[miss_col].isnull()].values
    # Model training
    X = known[:, :-1]
    y = known[:, -1]
    rfr = RandomForestRegressor(random_state=2, n_estimators=200, max_depth=4, n_jobs=-1)
    rfr.fit(X, y)
    predicted = rfr.predict(unknown[:, :-1]).round(0).astype(int)
    predicted = class_le.inverse_transform(predicted)
    # print(predicted)
    # Fill in the missing data with the predicted results obtained
    df.loc[(df[miss_col].isnull()), miss_col] = predicted
    return df

set_missing(data, estimate, 'BMI')
set_missing(data, estimate, 'X16')
set_missing(data, estimate, 'IL-6')
set_missing(data, estimate, 'X20')
set_missing(data, estimate, 'X21')
set_missing(data, estimate, 'hs-CRP')
set_missing(data, estimate, 'pH Value')
set_missing(data, estimate, 'PaO2')
set_missing(data, estimate, 'Magnesium Ions')
set_missing(data, estimate, 'X29')

data.isnull().any()
data.describe()

'''
Feature Engineering
'''

# Data standardization
X = data.iloc[:, :-1]
y = data.iloc[:, -1]
feature_name = X.columns
scaler = StandardScaler()
X.iloc[:, 7:] = scaler.fit_transform(X.iloc[:, 7:])

# Class balancing with SMOTE
from collections import Counter
print(Counter(y))
smote = SMOTE(random_state=1)
X_resampled, y_resampled = smote.fit_resample(X, y)
print(Counter(y_resampled))

'''
Recursive Feature Elimination
'''
from sklearn.feature_selection import RFECV

# Create a random forest classifier
RFC = RandomForestClassifier(n_estimators=100, random_state=42)
rfecv = RFECV(estimator=RFC, step=1, cv=5, scoring='accuracy')
rfecv.fit(X, y)

# Print the number of selected features  
print("Optimal number of features : %d" % rfecv.n_features_) 

# Obtain feature names and importance
best_features = rfecv.support_
best_feature_importance = rfecv.estimator_.feature_importances_[:rfecv.n_features_]
feature_names = X.columns[best_features]

# Generate sorted feature names and importance
sorted_indices = np.argsort(best_feature_importance)
sorted_features = feature_names[sorted_indices]
sorted_importance = best_feature_importance[sorted_indices]

# Printing feature importance ranking
print("Feature Importance Ranking:", sorted_features)  

# Visualize the performance of RFECV at every step
plt.figure(figsize=(10, 6))  
plt.xlabel("Number of Selected Features")  
plt.ylabel("Cross-validation Score")   
  
# Obtain the score for cross validation (mean_test_store)
n_features = np.arange(1, len(rfecv.cv_results_['mean_test_score']) + 1)  
scores = rfecv.cv_results_['mean_test_score']  
plt.plot(n_features, scores, marker='o')  
plt.scatter(rfecv.n_features_ + 1, np.max(scores), c='red', marker='o', label='Best Step')
plt.legend()  
plt.title("RFECV - Feature Selection")  
plt.grid(True)  
plt.show()  

# # Feature importance visualization
selected_features = X.columns[rfecv.support_]
importances = rfecv.estimator_.feature_importances_
sorted_idx = importances.argsort()

plt.figure(figsize=(12, 8))
colors = plt.cm.viridis(np.linspace(0, 1, len(selected_features)))
plt.barh(range(len(sorted_idx)), importances[sorted_idx], color=colors, edgecolor='black')
plt.yticks(range(len(sorted_idx)), selected_features[sorted_idx], fontsize=10)
plt.xlabel("Feature Importance (Gini Index)", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.title("Ranked Feature Importance with Scores", fontsize=14)
plt.grid(axis='x', alpha=0.5)

for i, v in enumerate(importances[sorted_idx]):
    plt.text(v + 0.005, i, f"{v:.3f}", color='black', va='center', fontsize=9)

plt.tight_layout()
plt.show()

# Obtain the optimal subset of features
# Obtain the Boolean mask of the selected features
feature_names = X.columns
feature_mask = rfecv.support_

# Obtain the name of the selected feature
selected_features = [feature for feature, mask in zip(feature_names, feature_mask) if mask]

# Print the selected feature names
print("Selected Features:", selected_features)

# Select important features
X_selected = rfecv.transform(X)
X_selected = pd.DataFrame(X_selected)
X_selected.columns = selected_features

'''
Model Development
'''

# Dataset Splitting
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y
)

# Initialize classifiers

classifiers = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'SVM': CalibratedClassifierCV(SVC(random_state=42, probability=True)),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Naive Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)
}
# Hyperparameter grids
param_grids = {
    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 5, 10]},
    'Logistic Regression': {'C': [0.1, 1, 10], 'penalty': ['l2']},
    'SVM': {'estimator__C': [0.1, 1, 10], 'estimator__kernel': ['linear', 'rbf']},
    'KNN': {'n_neighbors': [3, 5, 7]},
    'Decision Tree': {'max_depth': [None, 5, 10]},
    'Naive Bayes': {},
    'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 5]}
}

'''
Model Evaluation
'''
# Performance metrics initialization
def macro_roc_auc(y_true, y_proba):
    if y_proba.shape[1] == 1 or np.isnan(y_proba).any():
        return np.nan
    try:
        return roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
    except Exception as e:
        print(f"ROC calculation error: {str(e)}")
        return np.nan

def macro_pr_auc(y_true, y_proba):
    n_classes = y_proba.shape[1]
    aucs = []
    for i in range(n_classes):
        y_true_bin = (y_true == i).astype(int)
        if np.sum(y_true_bin) == 0:
            continue  
        try:
            precision, recall, _ = precision_recall_curve(y_true_bin, y_proba[:, i])
            aucs.append(auc(recall, precision))
        except Exception as e:
            print(f"PR calculation error (class{i}): {str(e)}")
    return np.mean(aucs) if aucs else np.nan

def bootstrap_ci(y_true, y_pred, y_proba, metric_func, n_bootstrap=1000, ci=95):
    scores = []
    n_samples = len(y_true)
    classes = np.unique(y_true)
    is_multiclass = len(classes) > 2

    for _ in range(n_bootstrap):
        indices = []
        # Stratified sampling ensures that each category has a sample
        for cls in classes:
            cls_indices = np.where(y_true == cls)[0]
            if len(cls_indices) == 0:
                continue
            # Calculate the number of samples that should be taken for this category (proportionally)
            cls_ratio = len(cls_indices) / n_samples
            n_cls_samples = max(1, int(n_samples * cls_ratio))
            cls_sampled = resample(
                cls_indices,
                replace=True,
                n_samples=n_cls_samples,
                random_state=42 + _
            )
            indices.extend(cls_sampled)
        indices = np.random.permutation(indices)  

        # validity check
        if len(indices) == 0:
            continue

        try:
            # Calculate metrics
            if metric_func.__name__ in ['macro_roc_auc', 'macro_pr_auc']:
                y_true_subset = y_true.iloc[indices]
                y_proba_subset = y_proba[indices]
                score = metric_func(y_true_subset, y_proba_subset)
            else:
                y_true_subset = y_true.iloc[indices]
                y_pred_subset = y_pred[indices]
                score = metric_func(y_true_subset, y_pred_subset)

            if not np.isnan(score):
                scores.append(score)
        except Exception as e:
            continue

    # Calculate confidence interval
    if len(scores) < 5:
        return "N/A (insufficient sample)"
    lower = np.percentile(scores, (100 - ci) / 2)
    upper = np.percentile(scores, ci + (100 - ci) / 2)
    return f"{lower:.2f}-{upper:.2f}"

# Model Training and Evaluation
accuracies = {
    'Model': [], 'Accuracy': [], 'Accuracy CI': [],
    'Precision': [], 'Precision CI': [],
    'Recall': [], 'Recall CI': [],
    'F1': [], 'F1 CI': [],
    'AUC-ROC': [], 'AUC-ROC CI': [],
    'AUC-PR': [], 'AUC-PR CI': [],
    'Brier Score': [],  
    'ECE': []           
}
# Calibration curve data
calibration_data = {}
best_models = {}

for model_name, model in classifiers.items():
    print(f"\nTraining {model_name}...")
    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_models[model_name] = best_model
    
    y_proba = best_model.predict_proba(X_test)
    y_pred = best_model.predict(X_test)
    
    # Calculate metrics
    accuracies['Model'].append(model_name)
    accuracies['Accuracy'].append(f"{accuracy_score(y_test, y_pred):.2f}")
    accuracies['Precision'].append(f"{precision_score(y_test, y_pred, average='macro'):.2f}")
    accuracies['Recall'].append(f"{recall_score(y_test, y_pred, average='macro'):.2f}")
    accuracies['F1'].append(f"{f1_score(y_test, y_pred, average='macro'):.2f}")
    accuracies['AUC-ROC'].append(f"{macro_roc_auc(y_test, y_proba):.2f}")
    accuracies['AUC-PR'].append(f"{macro_pr_auc(y_test, y_proba):.2f}")
    
    # Calculate confidence interval
    accuracies['Accuracy CI'].append(bootstrap_ci(y_test, y_pred, None, accuracy_score))
    accuracies['Precision CI'].append(bootstrap_ci(y_test, y_pred, None, lambda yt, yp: precision_score(yt, yp, average='macro')))
    accuracies['Recall CI'].append(bootstrap_ci(y_test, y_pred, None, lambda yt, yp: recall_score(yt, yp, average='macro')))
    accuracies['F1 CI'].append(bootstrap_ci(y_test, y_pred, None, lambda yt, yp: f1_score(yt, yp, average='macro')))
    accuracies['AUC-ROC CI'].append(bootstrap_ci(y_test, None, y_proba, macro_roc_auc))
    accuracies['AUC-PR CI'].append(bootstrap_ci(y_test, None, y_proba, macro_pr_auc))

    # calibration metrics
    brier = multiclass_brier(y_test, y_proba)
    ece = compute_ece((y_test == y_pred).astype(int), np.max(y_proba, axis=1))
    accuracies['Brier Score'].append(f"{brier:.2f}")
    accuracies['ECE'].append(f"{ece:.2f}")
    
    # Store calibration curves
    prob_true, prob_pred = calibration_curve(
        (y_test == y_pred).astype(int), 
        np.max(y_proba, axis=1), 
        n_bins=10, 
        strategy='quantile'
    )
    calibration_data[model_name] = (prob_true, prob_pred)

# Validate metric lengths
for k, v in accuracies.items():
    print(f"{k}: {len(v)}")  # All lengths should be equal here
    assert len(v) == len(classifiers), "Metric lengths inconsistent!"

# Store performance metrics
results_df = pd.DataFrame(accuracies)
try:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'Model Performance Results Table_{timestamp}.xlsx'
    desktop = Path.home() / "Desktop"
    save_path = desktop / filename
    results_df.to_excel(save_path, index=False)
    print(f"\nResult saved to：{save_path}")
except PermissionError:
    temp_dir = Path(os.environ['TEMP'])
    temp_save_path = temp_dir / filename
    results_df.to_excel(temp_save_path, index=False)
    print(f"\nWarning: Desktop save failed, saved to temporary directory：{temp_save_path}")
except Exception as e:
    results_df.to_csv('Model Performance Results Backup.csv', index=False)
    print(f"\nError：{e}，saved as CSV format")

'''
Result Visualization
'''
plt.rcParams['font.size'] = 12
colors = plt.cm.tab10(np.linspace(0, 1, len(classifiers)))
linestyles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1)), (0, (5, 5)), (0, (5, 1))]

# ROC Curve Comparison
plt.figure(figsize=(10, 8))
for (model_name, _), color, ls in zip(classifiers.items(), colors, linestyles):
    best_model = best_models[model_name]
    
    # Calculate macro average ROC
    y_proba = best_model.predict_proba(X_test)
    y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
    
    # Calculate the ROC curve for each category
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(3):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # Macro average ROC
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(3):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
    mean_tpr /= 3
    macro_auc = auc(all_fpr, mean_tpr)
    
    # Draw a curve and add confidence intervals
    plt.plot(all_fpr, mean_tpr, color=color, linestyle=ls,
             label=f'{model_name} (AUC = {macro_auc:.2f})',
             lw=2, alpha=0.8)

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('Comparison of ROC curves for 7 models (macro average)', fontsize=16)
plt.legend(loc='lower right', frameon=True, fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()

# PR Curve Comparison
plt.figure(figsize=(10, 8))
for (model_name, _), color, ls in zip(classifiers.items(), colors, linestyles):
    best_model = best_models[model_name]
    
    y_proba = best_model.predict_proba(X_test)
    y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
    
    # Calculate the macro average PR
    precision = dict()
    recall = dict()
    average_precision = dict()
    for i in range(3):
        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_proba[:, i])
        average_precision[i] = auc(recall[i], precision[i])
    
    # Macro average
    all_recall = np.unique(np.concatenate([recall[i] for i in range(3)]))
    mean_precision = np.zeros_like(all_recall)
    for i in range(3):
        mean_precision += np.interp(all_recall, recall[i][::-1], precision[i][::-1]) 
    mean_precision /= 3
    macro_pr = auc(all_recall, mean_precision)
    
    plt.plot(recall[i], precision[i], color=color, linestyle=ls,
             label=f'{model_name} (AP = {macro_pr:.2f})',
             lw=2, alpha=0.8)

plt.plot([0, 1], [1, 0], 'k--', lw=2)  
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Comparison of PR curves for 7 models (macro average)', fontsize=16)
plt.legend(loc='upper right', frameon=True, fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Calibration curve visualization
plt.figure(figsize=(10, 8))
colors = plt.cm.tab10(np.linspace(0, 1, len(classifiers)))

# Plot perfect calibration line
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')

# Plot calibration curves for each model
for idx, (model_name, (prob_true, prob_pred)) in enumerate(calibration_data.items()):
    plt.plot(
        prob_pred, 
        prob_true, 
        marker='o', 
        linestyle='-',
        color=colors[idx],
        label=f'{model_name} (Brier={accuracies["Brier Score"][idx]}, ECE={accuracies["ECE"][idx]})'
    )

# Plot styling
plt.xlabel('Mean predicted probability', fontsize=12)
plt.ylabel('Fraction of positives', fontsize=12)
plt.title('Model Calibration Comparison', fontsize=14)
plt.legend(loc='upper left', fontsize=10)
plt.grid(alpha=0.3)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.tight_layout()
plt.show()
'''
Model Explainability with SHAP and PDP
'''
xgb_model = best_models['XGBoost']

# 1. SHAP Summary Plot (Dot Plot)
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer(X_train)

# Choose class index (e.g., 2 for high-risk class)
class_idx = 2
class_shap = shap_values.values[:, :, class_idx]

# Create sorted feature list
mean_abs_shap = np.abs(class_shap).mean(0)
sorted_idx = np.argsort(mean_abs_shap)[::-1]
sorted_features = np.array(selected_features)[sorted_idx]
sorted_shap = class_shap[:, sorted_idx]
sorted_X = X_train.iloc[:, sorted_idx]

# Dynamic figure size
n_features = len(selected_features)
plt.figure(figsize=(8, n_features * 0.4 + 1))

# Generate SHAP plot
shap.summary_plot(
    sorted_shap,
    sorted_X,
    feature_names=sorted_features,
    max_display=n_features,
    plot_type="dot",
    show=False
)

# Customize color bar
cb = plt.gcf().axes[-1]
cb.set_yticks([0, 1])
cb.set_yticklabels(['Low', 'High'], fontsize=10)
cb.set_ylabel('')

# Add "Feature value" text
plt.figtext(0.92, 0.5, 'Feature value', 
            rotation=90, va='center', ha='center', 
            fontsize=10, fontweight='bold')

# Set axis labels
plt.gca().set_xlabel("SHAP value (impact on model output)", fontsize=10)
plt.gca().set_ylabel("")
plt.title("")

# Adjust layout
plt.tight_layout(rect=[0.05, 0.05, 0.9, 0.95])
plt.savefig('SHAP_Summary_Optimized.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. SHAP Bar Plot (Global Feature Importance)
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values,  # Use full Explanation object
    X_train,      # Use training data with selected features
    feature_names=selected_features,
    plot_type="bar",
    show=False
)
plt.title("SHAP Feature Importance (Bar Plot)", fontsize=16)
plt.tight_layout()
plt.savefig('SHAP_Bar.png', dpi=300)
plt.show()


# 3. Partial Dependence Plots (PDP)
# Get the actual top 3 features (most important)
top_features = sorted_features[-3:]

print("Top 3 features for PDP:")
print(top_features)

# Verify these features exist in selected_features
valid_features = [f for f in top_features if f in selected_features]
print("Valid features in selected set:")
print(valid_features)

# Create figure and subplots
fig, axs = plt.subplots(3, 3, figsize=(18, 15))
plt.subplots_adjust(hspace=0.4, wspace=0.3)

# Plot PDP for each feature and each class
for i, feature in enumerate(valid_features):
    for class_idx in range(3):  # Three classes
        try:
            # Plot partial dependence
            PartialDependenceDisplay.from_estimator(
                xgb_model, 
                X_train, 
                features=[feature],
                target=class_idx,
                feature_names=selected_features,
                ax=axs[class_idx, i],
                line_kw={"color": "red", "linewidth": 3},
                random_state=42  # Add for reproducibility
            )
            axs[class_idx, i].set_title(f'Class {class_idx}: PDP for {feature}', fontsize=12)
            axs[class_idx, i].set_xlabel(feature, fontsize=10)
            axs[class_idx, i].set_ylabel(f"P(class={class_idx})", fontsize=10)
            axs[class_idx, i].grid(alpha=0.3)
        except Exception as e:
            print(f"Error plotting {feature} for class {class_idx}: {str(e)}")
            axs[class_idx, i].set_title(f"Error: {feature}")
            axs[class_idx, i].text(0.5, 0.5, "Plot failed", 
                                    ha='center', va='center', 
                                    transform=axs[class_idx, i].transAxes)

# Handle case where we have less than 3 valid features
if len(valid_features) < 3:
    for i in range(len(valid_features), 3):
        for class_idx in range(3):
            axs[class_idx, i].axis('off')

plt.suptitle("Partial Dependence Plots for Top Features by Class", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.savefig('Partial_Dependence_Plots.png', dpi=300)
plt.show()

# 4. SHAP Dependence Plots
for feature in top_features:
    plt.figure(figsize=(10, 6))
    
    # Get feature index
    feature_idx = list(selected_features).index(feature)
    
    # Plot dependence for each class
    for class_idx in range(3): 
        # Get SHAP values for current class
        class_shap_values = shap_values.values[:, feature_idx, class_idx]
        
        # Plot scatter
        plt.scatter(
            X_train.iloc[:, feature_idx], 
            class_shap_values,
            alpha=0.5,
            label=f'Class {class_idx}'
        )
    
    plt.xlabel(feature, fontsize=12)
    plt.ylabel("SHAP Value", fontsize=12)
    plt.title(f"SHAP Dependence Plot for {feature}", fontsize=14)
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'SHAP_Dependence_{feature}.png', dpi=300)
    plt.show()